{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4bb2bde-3101-46be-95fc-a5143d6893ed",
   "metadata": {},
   "source": [
    "convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830e7d13-dd63-4626-b104-34384fad6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97366f8-8028-4313-a264-46978462b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ad2c6-f64f-4bc8-aa89-8ed08090c094",
   "metadata": {},
   "source": [
    "preprocessing the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c700ae-1341-4752-b271-47c68f5ec120",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datagen = ImageDataGenerator( \n",
    "    rescale= 1./255,\n",
    "    shear_range = 2.5,\n",
    "    zoom_range = 2.5,\n",
    "    horizontal_flip = True)\n",
    "\n",
    "\n",
    "#The ImageDataGenerator class in Keras is used for image preprocessing and augmentation. It applies real-time transformations to images, making the model more robust by providing varied versions of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded030bb-999b-4e52-94b5-0d740946af2b",
   "metadata": {},
   "source": [
    "Why Use ImageDataGenerator?\n",
    "Prevents overfitting by providing varied training samples.\n",
    "Expands the dataset size by creating augmented versions of images.\n",
    "Improves generalization to unseen data.\n",
    "\n",
    "1. rescale=1.0/255 = Pixel values in images range from 0 to 255.\n",
    "Dividing by 255 normalizes them to a range of 0 to 1, which helps in faster and stable training.\n",
    "2. shear_range=2.5 = Applies shear transformation, shifting image pixels in a diagonal direction.\n",
    "This helps the model generalize better to slight distortions.\n",
    "3. zoom_range=2.5 = Randomly zooms in or out on the image by up to 2.5x.\n",
    "Useful for training models to recognize objects at different scales.\n",
    "4. horizontal_flip=True = Randomly flips images left to right.\n",
    "Useful for cases like facial recognition, cats/dogs, where left-right orientation doesn’t matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b14df629-d922-436f-82d3-02ebeb1899d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 557 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = training_datagen.flow_from_directory(\n",
    "    'AI/cats_dogs/train',\n",
    "    target_size =(64,64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d32019-d87f-4fc5-b93a-13c30312338d",
   "metadata": {},
   "source": [
    "The flow_from_directory() method is used to load images from a directory and apply real-time data augmentation. It automatically labels images based on the folder structure and prepares them for training.\n",
    "\n",
    "1. 'PetImages'\n",
    "Specifies the path where images are stored.\n",
    "The directory must contain subfolders, each representing a class.\n",
    "\n",
    "2. target_size=(64,64)\n",
    "Resizes all images to 64x64 pixels (ensuring uniform input size).\n",
    "CNN models require fixed-size inputs.\n",
    "\n",
    "3. batch_size=32\n",
    "Specifies how many images to process at a time.\n",
    "A batch of 32 images is fed to the model in each step.\n",
    "Larger batches → Faster training (but needs more memory).\n",
    "\n",
    "4. class_mode='binary'\n",
    "For two classes (e.g., Cats & Dogs), use 'binary'.\n",
    "Labels will be 0 or 1.\n",
    "If more than two classes, use 'categorical' (one-hot encoding).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45521d-b9d0-4926-a801-21f04c276320",
   "metadata": {},
   "source": [
    "Preprocessing the traind set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af733e5-31b3-4eb1-a4e7-4fb94af9932f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 141 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255) #Creates a data generator for the test set\n",
    "test_set = test_datagen.flow_from_directory(        #Loads images from the directory and prepares them for the model\n",
    "    'AI/cats_dogs/test',\n",
    "    target_size =(64,64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29221f81-f7dd-46b6-a883-4a8a6b090718",
   "metadata": {},
   "source": [
    "This code is preprocessing test images using ImageDataGenerator and loading them from the cat_dog/test/ directory\n",
    "\n",
    "rescale=1./255: Normalizes pixel values from [0, 255] to [0, 1] for better model performance.\n",
    "No data augmentation (since test data should not be altered).\n",
    "\n",
    "('cat_dog/test/')\n",
    "target_size=(64,64) → Resizes all images to 150x150 pixels for consistency.\n",
    "batch_size=32 → Loads 32 images per batch for training.\n",
    "class_mode='binary' → Binary classification (e.g., Cat vs. Dog).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f909f7d-db90-47e8-9450-221c1a05754a",
   "metadata": {},
   "source": [
    "INITIALISING THE CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb1814a2-a4b3-4aaa-8b8e-564d56232903",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential() #This line of code creates a Convolutional Neural Network (CNN) model using TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3cdee-31a2-4ceb-b5f2-79184ec065de",
   "metadata": {},
   "source": [
    "CONVOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "925cb0bc-cec9-4c7c-a061-f921ef80d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer separately\n",
    "cnn.add(tf.keras.layers.Input(shape=(64, 64, 3)))  \n",
    "\n",
    "# Add convolutional layer (without input_shape)\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b3b04-a1b1-48ac-95a8-1ffdc7468c69",
   "metadata": {},
   "source": [
    "Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a284ec72-6e4b-4401-a092-af7c3102c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2)) #This adds a Max Pooling layer to your Convolutional Neural Network (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a270f8a8-d0f6-403c-aa80-b970408dc23e",
   "metadata": {},
   "source": [
    "1. MaxPool2D: Downsamples the feature maps → Reduces image dimensions while preserving important features.\n",
    "2. Improves computational efficiency → Reduces the number of parameters & speeds up training.\n",
    "3. Helps prevent overfitting → Removes unnecessary details while keeping key patterns.\n",
    "4. pool_size=2\tTakes a 2×2 window and selects the maximum value in that region.\n",
    "5. strides=2\tMoves the pooling window 2 pixels at a time, reducing the feature map size by half.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cdee54-58d9-43fc-9b0e-a5e635c83d1c",
   "metadata": {},
   "source": [
    "ADDING A SECOND CONVOLUTIONAL LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd6ed2ca-a5ff-4044-a8f9-6ad1679f0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu')) #do not add input shape for the second layer.\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359cccf-c0e5-433c-9348-934fa9050e5a",
   "metadata": {},
   "source": [
    "FLATTENING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "808a976d-143d-4d06-af45-0086c3cf9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten()) #This adds a Flatten layer to your Convolutional Neural Network (CNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500dda21-2b46-4f95-9d59-ea52acf76046",
   "metadata": {},
   "source": [
    "1. Converts a multi-dimensional feature map into a 1D vector\n",
    "2. Prepares data for the Dense (fully connected) layers\n",
    "3. Maintains important extracted features while making them suitable for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e975e675-2294-4051-9e07-d0714bbd0017",
   "metadata": {},
   "source": [
    "Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5a55b16-f0d2-4b1b-849e-cedfdd4ce0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units= 128, activation ='relu')) #This adds a fully connected (Dense) layer with 128 neurons and ReLU activation to your CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff94fb-2fd0-46ec-818a-21449ebf1913",
   "metadata": {},
   "source": [
    "What Does Dense() Do?\n",
    "1. Creates a fully connected layer, where every neuron is connected to all previous layer neurons.\n",
    "2. Processes extracted features from the convolutional layers for classification.\n",
    "3. Uses activation functions (like ReLU, Softmax) to model complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49a96d-0ec3-42f2-ae79-1219261de19f",
   "metadata": {},
   "source": [
    "Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8ee2ab7-9fad-4e7d-859e-fff12729485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units= 1, activation ='sigmoid')) #This adds an output layer with 1 neuron and Sigmoid activation for binary classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835e4da-2809-49c2-908b-bb30e944469f",
   "metadata": {},
   "source": [
    "TRAINING THE CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b44f9-988a-43d2-87ce-33731e087411",
   "metadata": {},
   "source": [
    "compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92527aeb-662a-4eb9-acf4-5013949ffcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer ='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfb822-565f-437b-afe4-e2f22b2e5803",
   "metadata": {},
   "source": [
    "training the CNN on the training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29fea920-cd02-457f-add9-b666e72de7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - accuracy: 0.4633 - loss: 0.8461 - val_accuracy: 0.4965 - val_loss: 0.6975\n",
      "Epoch 2/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.5291 - loss: 0.6887 - val_accuracy: 0.5745 - val_loss: 0.6911\n",
      "Epoch 3/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.5957 - loss: 0.6916 - val_accuracy: 0.5248 - val_loss: 0.6908\n",
      "Epoch 4/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.5166 - loss: 0.6964 - val_accuracy: 0.5248 - val_loss: 0.6917\n",
      "Epoch 5/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.5208 - loss: 0.6935 - val_accuracy: 0.5248 - val_loss: 0.6890\n",
      "Epoch 6/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.5545 - loss: 0.6888 - val_accuracy: 0.5035 - val_loss: 0.6942\n",
      "Epoch 7/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.5022 - loss: 0.6947 - val_accuracy: 0.5461 - val_loss: 0.6856\n",
      "Epoch 8/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.5910 - loss: 0.6669 - val_accuracy: 0.5248 - val_loss: 0.6889\n",
      "Epoch 9/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 111ms/step - accuracy: 0.5883 - loss: 0.6728 - val_accuracy: 0.5319 - val_loss: 0.7047\n",
      "Epoch 10/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.5534 - loss: 0.6839 - val_accuracy: 0.5035 - val_loss: 0.7006\n",
      "Epoch 11/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 0.6221 - loss: 0.6434 - val_accuracy: 0.5319 - val_loss: 0.6930\n",
      "Epoch 12/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.6284 - loss: 0.6732 - val_accuracy: 0.4965 - val_loss: 0.7491\n",
      "Epoch 13/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.5912 - loss: 0.6730 - val_accuracy: 0.5603 - val_loss: 0.7088\n",
      "Epoch 14/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6516 - loss: 0.6456 - val_accuracy: 0.5674 - val_loss: 0.7014\n",
      "Epoch 15/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6428 - loss: 0.6350 - val_accuracy: 0.5532 - val_loss: 0.7206\n",
      "Epoch 16/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.5947 - loss: 0.6582 - val_accuracy: 0.5745 - val_loss: 0.7197\n",
      "Epoch 17/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - accuracy: 0.6573 - loss: 0.6278 - val_accuracy: 0.6170 - val_loss: 0.7063\n",
      "Epoch 18/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - accuracy: 0.5656 - loss: 0.6597 - val_accuracy: 0.5745 - val_loss: 0.7172\n",
      "Epoch 19/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - accuracy: 0.6163 - loss: 0.6321 - val_accuracy: 0.5674 - val_loss: 0.7227\n",
      "Epoch 20/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 0.6667 - loss: 0.6145 - val_accuracy: 0.5603 - val_loss: 0.7372\n",
      "Epoch 21/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.6391 - loss: 0.6141 - val_accuracy: 0.6241 - val_loss: 0.6989\n",
      "Epoch 22/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 121ms/step - accuracy: 0.6298 - loss: 0.6173 - val_accuracy: 0.5887 - val_loss: 0.7510\n",
      "Epoch 23/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - accuracy: 0.6637 - loss: 0.6082 - val_accuracy: 0.5957 - val_loss: 0.6687\n",
      "Epoch 24/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.5656 - loss: 0.6612 - val_accuracy: 0.6312 - val_loss: 0.7302\n",
      "Epoch 25/25\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - accuracy: 0.6599 - loss: 0.6164 - val_accuracy: 0.5603 - val_loss: 0.7121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16becb470>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25) #This trains the CNN model using the provided dataset. It runs for 25 epochs, validating performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898247e7-0f78-465a-a78e-3e7c70faa843",
   "metadata": {},
   "source": [
    "MAKING A SINGLE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea5d5926-0034-446a-95a2-75cf12ee1f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image \n",
    "test_image = image.load_img('AI/cats_dogs/predict/pup2.jpg',target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'Dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "afd802de-0dd0-49c5-90c6-427ef4e6e970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
